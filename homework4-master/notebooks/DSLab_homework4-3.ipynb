{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLab Homework4 - More trains (PART III)\n",
    "\n",
    "## Hand-in Instructions:\n",
    "- __Due: 11.05.2021 23:59:59 CET__\n",
    "- your project must be private\n",
    "- git push your final verion to the master branch of your group's Renku repository before the due date\n",
    "- check if Dockerfile, environment.yml and requirements.txt are properly written\n",
    "- add necessary comments and discussion to make your codes readable\n",
    "\n",
    "## NS Streams\n",
    "For this homework, you will be working with the real-time streams of the NS, the train company of the Netherlands. You can see an example webpage that uses the same streams to display the train information on a map: https://spoorkaart.mwnn.nl/ . \n",
    "\n",
    "To help you and avoid having too many connections to the NS streaming servers, we have setup a service that collects the streams and pushes them to our Kafka instance. The related topics are: \n",
    "\n",
    "`ndovloketnl-arrivals`: For each arrival of a train in a station, describe the previous and next station, time of arrival (planned and actual), track number,...\n",
    "\n",
    "`ndovloketnl-departures`: For each departure of a train from a station, describe the previous and next station, time of departure (planned and actual), track number,...\n",
    "\n",
    "`ndovloketnl-gps`: For each train, describe the current location, speed, bearing.\n",
    "\n",
    "The events are serialized in JSON (actually converted from XML), with properties in their original language. Google translate could help you understand all of them, but we will provide you with some useful mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**PART III is in PySpark kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "ipython = get_ipython()\n",
    "print('Current kernel: {}'.format(ipython.kernel.kernel_info['implementation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Set up environment\n",
    "\n",
    "Run the following cells below before running the other cells of this notebook. Run them whenever you need to recreate a Spark context. Pay particular attention to your `username` settings, and make sure that it is properly set to your user name, both locally and on the remote Spark Driver.\n",
    "\n",
    "Configure your spark settings:\n",
    "1. name your spark application as `\"<your_gaspar_id>-homework4\"`.\n",
    "2. make the required kafka jars available on the remote Spark driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "configuration = dict(\n",
    "    name = \"{}-homework4\".format(username),\n",
    "    executorMemory = \"1G\",\n",
    "    executorCores = 2,\n",
    "    numExecutors = 2,\n",
    "    conf = {\n",
    "        \"spark.jars.packages\":\"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.2,org.apache.kafka:kafka_2.11:1.0.1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new session unless one was already created above (check for `âœ”` in current session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send `username` to the Spark driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You are {} on the Spark driver.\".format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Kafka client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "from pykafka.common import OffsetType\n",
    "\n",
    "ZOOKEEPER_QUORUM = 'iccluster040.iccluster.epfl.ch:2181,' \\\n",
    "                   'iccluster064.iccluster.epfl.ch:2181,' \\\n",
    "                   'iccluster065.iccluster.epfl.ch:2181'\n",
    "\n",
    "client = KafkaClient(zookeeper_hosts=ZOOKEEPER_QUORUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on data streams is often times more complex compared to using static datasets, so we will first look at how to create static RDDs for easy prototyping.\n",
    "\n",
    "You can find below a function that creates a static RDD from a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def simple_create_rdd(topic, from_offset, to_offset):\n",
    "    \"\"\"Create an RDD from topic with offset in [from_offset, to_offest).\"\"\"\n",
    "    \n",
    "    consumer = client.topics[topic].get_simple_consumer(\n",
    "        auto_offset_reset=OffsetType.EARLIEST if from_offset == 0 else from_offset - 1,\n",
    "        reset_offset_on_start=True\n",
    "    )\n",
    "    \n",
    "    return sc.parallelize((msg.offset, msg.value) for msg in islice(consumer, to_offset - from_offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this function, we need to retrieve valid offsets from Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = client.topics[b'ndovloketnl-arrivals']\n",
    "topic.earliest_available_offsets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can for example retrieve the first 1000 messages from the topic `ndovloketnl-arrivals`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = topic.earliest_available_offsets()[0].offset[0]\n",
    "rdd = simple_create_rdd(b'ndovloketnl-arrivals', offset, offset+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint folder\n",
    "checkpoint = 'hdfs:///user/{}/checkpoint/'.format(username)\n",
    "print('checkpoint created at hdfs:///user/{}/checkpoint/'.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a StreamingContext with two working thread and batch interval of 5 seconds.\n",
    "# Each time you stop a StreamingContext, you will need to recreate it.\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(checkpoint)\n",
    "\n",
    "group_id = 'ns-{0}'.format(username)\n",
    "\n",
    "# Input streams\n",
    "arrival_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, {'ndovloketnl-arrivals': 1})\n",
    "departure_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, {'ndovloketnl-departures': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just print the content of the streams. Because we set the batch interval as 10 seconds and the timeout also as 10 seconds, you are supposed to see exact one batch from each stream, like:\n",
    "```\n",
    "-------------------------------------------\n",
    "Time: 2021-04-27 10:11:50\n",
    "-------------------------------------------\n",
    "<ONE_BATCH_OF_ARRIVAL_STREAM>\n",
    "...\n",
    "-------------------------------------------\n",
    "Time: 2021-04-27 10:11:50\n",
    "-------------------------------------------\n",
    "<ONE_BATCH_OF_DEPARTURE_STREAM>\n",
    "...\n",
    "```\n",
    "**Note:** the output may be shown after you run `ssc.stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrival_stream.pprint(num=2) # print the first 2 messages\n",
    "departure_stream.pprint(num=2) # print the first 2 messages\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to adjust the batch interval (10 seconds here) in accordance with the processing times. Use the spark UI to check if batches are not accumulating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III - Live stopping time (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will have a look at the two other streams, namely `ndovloketnl-arrivals` and `ndovloketnl-departures`. Each time a train arrives at or leaves a station, a message is generated. Let's have a look at the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pykafka.common import OffsetType\n",
    "\n",
    "example_arrivals = client.topics[b'ndovloketnl-arrivals'].get_simple_consumer(\n",
    "    auto_offset_reset=OffsetType.EARLIEST,\n",
    "    reset_offset_on_start=True\n",
    ").consume()\n",
    "print(json.dumps(json.loads(example_arrivals.value), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_departures = client.topics[b'ndovloketnl-departures'].get_simple_consumer(\n",
    "    auto_offset_reset=OffsetType.EARLIEST,\n",
    "    reset_offset_on_start=True\n",
    ").consume()\n",
    "print(json.dumps(json.loads(example_departures.value), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the messages have the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "  'ns1:PutReisInformatieBoodschapIn': {\n",
    "    'ns2:ReisInformatieProductDVS' or 'ns2:ReisInformatieProductDAS': {\n",
    "      'ns2:DynamischeVertrekStaat' or 'ns2:DynamischeAankomstStaat': {\n",
    "          'ns2:RitStation': <station_info>,\n",
    "          'ns2:Trein' or 'ns2:TreinAankomst': {\n",
    "              'ns2:VertrekTijd' or 'ns2:AankomstTijd': [<planned_and_actual_times>],\n",
    "              'ns2:TreinNummer': <train_number>,\n",
    "              'ns2:TreinSoort': <kind_of_train>,\n",
    "              ...\n",
    "          }\n",
    "           \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We can see also that the train stations have a long name `ns2:LangeNaam`, a medium name `ns2:MiddelNaam`, a short name `ns2:KorteNaam`, a station code `ns2:StationCode` and a kind of nummerical ID `ns2:UICCode`. When giving information about times, tracks, direction,... you will find sometimes the information twice with the status `Gepland` (which means planned, according to the schedule) and `Actueel`(which means the actual measured value). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Parse - 5/20 \n",
    "\n",
    "We want to compute the time a train stays at a station and get a real-time histogram for a given time window. To begin with, you need to write some parsing functions that will allow you to get information from the data streams. We have prepare one function `parse_train_dep` for the stream `ndovloketnl-departures`, which returns a Key-Value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_train_dep(s):\n",
    "    obj = json.loads(s)\n",
    "    tn = (obj.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "             .get('ns2:ReisInformatieProductDVS', {})\n",
    "             .get('ns2:DynamischeVertrekStaat', {})\n",
    "             .get('ns2:Trein', {})\n",
    "             .get(\"ns2:TreinNummer\"))\n",
    "    st = (obj.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "             .get('ns2:ReisInformatieProductDVS', {})\n",
    "             .get('ns2:DynamischeVertrekStaat', {})\n",
    "             .get('ns2:RitStation', {})\n",
    "             .get(\"ns2:UICCode\"))\n",
    "    if tn and st:\n",
    "        return [(\"{}-{}\".format(tn, st), obj)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_train_dep(example_departures.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO - 1/5__ Please check the function `parse_train_dep` above. Explain how we construct the Key and the Value, and why we construct them in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** According to parse outcome of the function, we extract train number, station number and the corresponding information. the return('train_number'-'station_number':'msg') is a (key:value) format, where the key refers a stop of a train at a station. Such construction can help us extract the departure times for each train at the stop station from streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO - 2/5__  Take `parse_train_dep` as an example and write the function `parse_train_arr` for the stream `ndovloketnl-arrivals`. Make sure they have the same output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_train_arr(s):\n",
    "    \n",
    "    obj = json.loads(s)\n",
    "    tn = (obj.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "             .get('ns2:ReisInformatieProductDAS', {})\n",
    "             .get('ns2:DynamischeAankomstStaat', {})\n",
    "             .get('ns2:TreinAankomst', {})\n",
    "             .get(\"ns2:TreinNummer\"))\n",
    "    st = (obj.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "             .get('ns2:ReisInformatieProductDAS', {})\n",
    "             .get('ns2:DynamischeAankomstStaat', {})\n",
    "             .get('ns2:RitStation', {})\n",
    "             .get(\"ns2:UICCode\"))\n",
    "    if tn and st:\n",
    "        return [(\"{}-{}\".format(tn, st), obj)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_train_arr(example_arrivals.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO - 2/5__ Another parsing function you will need later is `get_actual_time`, which will allow you to extract the **actual** time from the fields of time information, which are `ns2:AankomstTijd` in the arrival stream and `ns2:VertrekTijd` in the departure stream. \n",
    "\n",
    "__Note:__ These two fields may be empty and they may not contain the actual time information. In both cases the function should return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_actual_time(tab):\n",
    "    \n",
    "    # Done\n",
    "    actual_time = None\n",
    "    for times in tab:\n",
    "        if times['@InfoStatus'.encode()] == 'Actueel'.encode():\n",
    "            actual_time = str(times['#text'.encode()])\n",
    "            actual_time = datetime.datetime.strptime(actual_time, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            \n",
    "    return actual_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the field of time in the departure stream\n",
    "example_dep_json = json.loads(example_departures.value)\n",
    "example_dep_tab = (example_dep_json.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "                                   .get(\"ns2:ReisInformatieProductDVS\", {})\n",
    "                                   .get(\"ns2:DynamischeVertrekStaat\", {})\n",
    "                                   .get(\"ns2:Trein\", {})\n",
    "                                   .get(\"ns2:VertrekTijd\",{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_actual_time(example_dep_tab) == datetime.datetime(2021, 4, 28, 12, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the field of time in the arrival stream\n",
    "example_arr_json = json.loads(example_arrivals.value)\n",
    "example_arr_tab = (example_arr_json.get('ns1:PutReisInformatieBoodschapIn', {}) # Done\n",
    "                                   .get(\"ns2:ReisInformatieProductDAS\", {})\n",
    "                                   .get(\"ns2:DynamischeAankomstStaat\", {})\n",
    "                                   .get(\"ns2:TreinAankomst\", {})\n",
    "                                   .get(\"ns2:AankomstTijd\",{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_actual_time(example_arr_tab) == datetime.datetime(2021, 4, 26, 11, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Transform - 5/20\n",
    "\n",
    "Create two Spark streams from the arrivals and departures where the records are in the form (Key, Value) using `parse_train_dep` and  `parse_train_arr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a StreamingContext with two working thread and batch interval of 10 seconds.\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint('hdfs:///user/{}/checkpoint/'.format(username))\n",
    "\n",
    "group_id = 'ns-{0}'.format(username)\n",
    "\n",
    "# Input streams\n",
    "arrival_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, {'ndovloketnl-arrivals': 1})\n",
    "departure_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, {'ndovloketnl-departures': 1})\n",
    "\n",
    "arrival_stream = arrival_stream.flatMap(lambda x: parse_train_arr(x[1]))\n",
    "departure_stream = departure_stream.flatMap(lambda x:parse_train_dep(x[1]))\n",
    "\n",
    "#arrival_stream.pprint()\n",
    "#departure_stream.pprint()\n",
    "\n",
    "#ssc.start()\n",
    "#ssc.awaitTermination(timeout=10)\n",
    "#ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Window and Join - 5/20\n",
    "\n",
    "Every 20 seconds, we want to have a list of trains that had departed from any train station after staying for 5 minutes or less at the station. Apply a window of 20s sliding interval on arrival and departure streams. Join two streams such that trains staying for 5 minutes or less (Â± 20 seconds error due to sliding interval) at any station are caught in the RDD window of the joined stream (you can ignore late messages). \n",
    "\n",
    "__Note:__\n",
    "- Check [here](https://spark.apache.org/docs/2.3.2/streaming-programming-guide.html#window-operations) for windowed computations in Spark Streaming.\n",
    "- Use the methods [reduceByKeyAndWindow](https://spark.apache.org/docs/2.3.2/api/python/pyspark.streaming.html?highlight=reducebykey#pyspark.streaming.DStream.reduceByKeyAndWindow) and [join](https://spark.apache.org/docs/2.3.2/api/python/pyspark.streaming.html?highlight=reducebykey#pyspark.streaming.DStream.join) on DStream objects.\n",
    "- Both windows should have `slideDuration` of 20s\n",
    "- You have to pick the sizes of windows `windowDuration` carefully. The sizes can be different: \n",
    "    - The trains staying for 5 minutes or less (Â± 20 seconds error due to sliding interval) must be in the joined stream.\n",
    "    - A same stay (i.e. one train at one station) is caught in the joined stream once and only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create a StreamingContext with two working thread and batch interval of 20 seconds.\n",
    "#ssc = StreamingContext(sc, 20)\n",
    "#ssc.checkpoint('hdfs:///user/{}/checkpoint/'.format(username))\n",
    "\n",
    "#group_id = 'ns-{0}'.format(username)\n",
    "\n",
    "# # Input streams\n",
    "#arrival_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, {'ndovloketnl-arrivals': 1})\n",
    "#departure_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, {'ndovloketnl-departures': 1})\n",
    "\n",
    "#arrival_stream = arrival_stream.flatMap(lambda x: parse_train_arr(x[1]))\n",
    "#departure_stream = departure_stream.flatMap(lambda x: parse_train_dep(x[1]))\n",
    "\n",
    "arrival_stream = arrival_stream.reduceByKeyAndWindow(lambda x, y: x, 300, 20)\n",
    "departure_stream = departure_stream.reduceByKeyAndWindow(lambda x, y: x, 20, 20)\n",
    "joined_stream = departure_stream.join(arrival_stream)\n",
    "\n",
    "#joined_stream = joined_stream.map(lambda x: x[0])\n",
    "#joined_stream.pprint()\n",
    "#ssc.start()\n",
    "#ssc.awaitTermination(timeout=60)\n",
    "#ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Histogram - 5/20\n",
    "\n",
    "On the joined stream, compute the length of each stay (you can round to the minute) and produce a stream of histograms. You don't need to plot them, a value/count array is enough, like:\n",
    "```\n",
    "-------------------------------------------\n",
    "Time: 2018-05-17 11:10:00\n",
    "-------------------------------------------\n",
    "(0.0, 110)\n",
    "(4.0, 3)\n",
    "(8.0, 2) # introduced by late messages\n",
    "\n",
    "-------------------------------------------\n",
    "Time: 2018-05-17 11:10:20\n",
    "-------------------------------------------\n",
    "(0.0, 46)\n",
    "(4.0, 2)\n",
    "(1.0, 5)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_stay(departure_value, arrival_value):\n",
    "    \n",
    "    departure_time = get_actual_time((departure_value.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "                                                     .get(\"ns2:ReisInformatieProductDVS\", {})\n",
    "                                                     .get(\"ns2:DynamischeVertrekStaat\", {})\n",
    "                                                     .get(\"ns2:Trein\", {})\n",
    "                                                     .get(\"ns2:VertrekTijd\",{})\n",
    "                                     ))\n",
    "    arrival_time = get_actual_time((arrival_value.get('ns1:PutReisInformatieBoodschapIn', {})\n",
    "                                                 .get(\"ns2:ReisInformatieProductDAS\", {})\n",
    "                                                 .get(\"ns2:DynamischeAankomstStaat\", {})\n",
    "                                                 .get(\"ns2:TreinAankomst\", {})\n",
    "                                                 .get(\"ns2:AankomstTijd\",{})\n",
    "                                   ))\n",
    "    \n",
    "    difference = departure_time - arrival_time\n",
    "    return round(difference.seconds/60., 1)\n",
    "    \n",
    "histograms_stream = joined_stream.map(lambda x: (compute_stay(x[1][0], x[1][1]), 1))\n",
    "histograms_stream = histograms_stream.reduceByKey(lambda x,y: x+y)\n",
    "histograms_stream.pprint(num=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start() \n",
    "ssc.awaitTermination(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
