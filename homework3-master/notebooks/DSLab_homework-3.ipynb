{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLab Homework3 - Uncovering World Events using Twitter Hashtags\n",
    "\n",
    "## ... and learning about Spark `DataFrames` along the way\n",
    "\n",
    "In this notebook, we will use temporal information about Twitter hashtags to discover trending topics and potentially uncover world events as they occurred. \n",
    "\n",
    "## Hand-in Instructions:\n",
    "\n",
    "- __Due: 27.04.2021 23:59:59 CET__\n",
    "- your project must be private\n",
    "- `git push` your final verion to the master branch of your group's Renku repository before the due date\n",
    "- check if `Dockerfile`, `environment.yml` and `requirements.txt` are properly written\n",
    "- add necessary comments and discussion to make your codes readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags\n",
    "\n",
    "The idea here is that when an event is happening and people are having a conversation about it on Twitter, a set of uniform hashtags that represent the event spontaneously evolves. Twitter users then use those hashtags to communicate with one another. Some hashtags, like `#RT` for \"retweet\" or just `#retweet` are used frequently and don't tell us much about what is going on. But a sudden appearance of a hashtag like `#oscars` probably indicates that the oscars are underway. For a particularly cool example of this type of analysis, check out [this blog post about earthquake detection using Twitter data](https://blog.twitter.com/official/en_us/a/2015/usgs-twitter-data-earthquake-detection.html) (although they search the text and not necessarily hashtags)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell='{ \"name\":\"%s-homework3\", \"executorMemory\":\"4G\", \"executorCores\":4, \"numExecutors\":10, \"driverMemory\": \"4G\" }' % username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send `username` to Saprk kernel, which will frist start the Spark application if there is no active session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Set up (5 points)\n",
    "\n",
    "The twitter stream data is downloaded from [Archive Team: The Twitter Stream Grab](https://archive.org/details/twitterstream), which is a collection of a random sample of all tweets. We have parsed the stream data and prepared the twitter hashtag data of __2020__, a very special and different year in many ways. Let's see if we can see any trends about all these events of 2020 in the Twitter data. \n",
    "\n",
    "<div style=\"font-size: 100%\" class=\"alert alert-block alert-danger\">\n",
    "<b>Disclaimer</b>\n",
    "<br>\n",
    "This dataset contains unfiltered data from Twitter. As such, you may be exposed to tweets/hashtags containing vulgarities, references to sexual acts, drug usage, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Load data - 1/10\n",
    "\n",
    "Load the **orc** data from `/data/twitter/orc/hashtags/` into a Spark dataframe using the appropriate `SparkSession` method. \n",
    "\n",
    "Look at the first few rows of the dataset - note the timestamp and its units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# df = spark.<your_code>\n",
    "# Load the data\n",
    "df = spark.read.orc(\"/data/twitter/orc/hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the first 5 line\n",
    "df.show(n=5, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 100%\" class=\"alert alert-block alert-info\">\n",
    "    <b>Cluster Usage:</b> As there are many of you working with the cluster, we encourage you to\n",
    "    <ul>\n",
    "        <li>prototype your queries on small data samples before running them on whole datasets</li>\n",
    "        <li>save your intermediate results in your own directory at hdfs <b>\"/user/&lt;your-gaspar-id&gt;/\"</b></li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "    # create a subset of original dataset\n",
    "    df_sample = df.sample(0.01)\n",
    "    \n",
    "    # save as orc\n",
    "    df_sample.write.orc('/user/%s/sample.orc' % username, mode='overwrite')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Functions - 2/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__User-defined functions__\n",
    "\n",
    "A neat trick of spark dataframes is that you can essentially use something very much like an RDD `map` method but without switching to the RDD. If you are familiar with database languages, this works very much like e.g. a user-defined function in SQL. \n",
    "\n",
    "So, for example, if we wanted to make a user-defined python function that returns the hashtags in lowercase, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def lowercase(text):\n",
    "    \"\"\"Convert text to lowercase\"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@F.udf` is a \"decorator\" -- this is really handy python syntactic sugar and in this case is equivalent to:\n",
    "\n",
    "```python\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "    \n",
    "lowercase = F.udf(lowercase)\n",
    "```\n",
    "\n",
    "It basically takes our function and adds to its functionality. In this case, it registers our function as a pyspark dataframe user-defined function (UDF).\n",
    "\n",
    "Using these UDFs is very straightforward and analogous to other Spark dataframe operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(lowercase(df.hashtag)).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Built-in functions__\n",
    "\n",
    "Using a framework like Spark is all about understanding the ins and outs of how it functions and knowing what it offers. One of the cool things about the dataframe API is that many functions are already defined for you (turning strings into lowercase being one of them). Find the [Spark python API documentation](https://spark.apache.org/docs/2.3.2/api/python/index.html). Look for the `sql` section and find the listing of `sql.functions`. Repeat the above (turning hashtags into lowercase) but use the built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# df.<your_code>\n",
    "from pyspark.sql.functions import lower\n",
    "# turning hashtags into lowercase\n",
    "df_lowercase = df.withColumn(\"hashtag\", lower(df.hashtag))\n",
    "df_lowercase.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with a combination of these built-in functions and user-defined functions for the remainder of this homework. \n",
    "\n",
    "Note that the functions can be combined. Consider the following dataframe and its transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a sample dataframe with one column \"degrees\" going from 0 to 180\n",
    "test_df = spark.createDataFrame(spark.sparkContext.range(180).map(lambda x: Row(degrees=x)), ['degrees'])\n",
    "\n",
    "# define a function \"sin_rad\" that first converts degrees to radians and then takes the sine using built-in functions\n",
    "sin_rad = F.sin(F.radians(test_df.degrees))\n",
    "\n",
    "# show the result\n",
    "test_df.select(sin_rad).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Tweets in english - 2/10\n",
    "\n",
    "- Create `english_df` with only english-language tweets. \n",
    "- Turn hashtags into lowercase.\n",
    "- Convert the timestamp to a more readable format and name the new column as `date`.\n",
    "- Sort the table in chronological order. \n",
    "\n",
    "Your `english_df` should look something like this:\n",
    "\n",
    "```\n",
    "+-----------+----+-----------+-------------------+\n",
    "|timestamp_s|lang|    hashtag|               date|\n",
    "+-----------+----+-----------+-------------------+\n",
    "| 1577862000|  en| spurfamily|2020-01-01 08:00:00|\n",
    "| 1577862000|  en|newyear2020|2020-01-01 08:00:00|\n",
    "| 1577862000|  en|     master|2020-01-01 08:00:00|\n",
    "| 1577862000|  en|  spurrific|2020-01-01 08:00:00|\n",
    "| 1577862000|  en|     master|2020-01-01 08:00:00|\n",
    "+-----------+----+-----------+-------------------+\n",
    "```\n",
    "\n",
    "__Note:__ \n",
    "- The hashtags may not be in english.\n",
    "- [pyspark.sql.functions](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# english_df = df.<your_code>\n",
    "# select the data\n",
    "english_df = df_lowercase.filter(df_lowercase.lang == 'en')\n",
    "# order the data\n",
    "english_df = english_df.withColumn('date', F.from_unixtime('timestamp_s','yyyy-MM-dd hh:mm:ss')).orderBy('date')\n",
    "english_df = english_df.drop(english_df.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the english data\n",
    "english_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Twitter hashtag trends (30 points)\n",
    "\n",
    "In this section we will try to do a slightly more complicated analysis of the tweets. Our goal is to get an idea of tweet frequency as a function of time for certain hashtags. \n",
    "\n",
    "Have a look [here](http://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#module-pyspark.sql.functions) to see the whole list of custom dataframe functions - you will need to use them to complete the next set of TODO items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Top hashtags - 1/30\n",
    "\n",
    "We used `groupBy` already in the previous notebooks, but here we will take more advantage of its features. \n",
    "\n",
    "One important thing to note is that unlike other RDD or DataFrame transformations, the `groupBy` does not return another DataFrame, but a `GroupedData` object instead, with its own methods. These methods allow you to do various transformations and aggregations on the data of the grouped rows. \n",
    "\n",
    "Conceptually the procedure is a lot like this:\n",
    "\n",
    "![groupby](https://i.stack.imgur.com/sgCn1.jpg)\n",
    "\n",
    "The column that is used for the `groupBy` is the `key` - once we have the values of a particular key all together, we can use various aggregation functions on them to generate a transformed dataset. In this example, the aggregation function is a simple `sum`. In the simple procedure below, the `key` will be the hashtag.\n",
    "\n",
    "\n",
    "Use `groupBy`, calculate the top 5 most common hashtags in the whole english-language dataset.\n",
    "\n",
    "This should be your result:\n",
    "\n",
    "```\n",
    "+-----------------+-------+\n",
    "|          hashtag|  count|\n",
    "+-----------------+-------+\n",
    "|              bts|1200196|\n",
    "|          endsars|1019280|\n",
    "|          covid19| 717238|\n",
    "|            방탄소년단| 488160|\n",
    "|sarkaruvaaripaata| 480124|\n",
    "+-----------------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# english_df.<your_code>\n",
    "# calculate the top 5 most common hashtags in the whole english-language dataset.\n",
    "hashtag_count = english_df.groupBy('hashtag')\\\n",
    "                          .agg(F.count('hashtag').alias('count'))\\\n",
    "                          .sort(F.desc(('count')))\n",
    "hashtag_count.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Daily hashtags - 2/50\n",
    "\n",
    "Now, let's see how we can start to organize the tweets by their timestamps. Remember, our goal is to uncover trending topics on a timescale of a few days. A much needed column then is simply `day`. Spark provides us with some handy built-in dataframe functions that are made for transforming date and time fields.\n",
    "\n",
    "- Create a dataframe called `daily_hashtag` that includes the columns `month`, `week`, `day` and `hashtag`. \n",
    "- Use the `english_df` you made above to start, and make sure you find the appropriate spark dataframe functions to make your life easier. For example, to convert the date string into day-of-year, you can use the built-in [dayofyear](http://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#pyspark.sql.functions.dayofyear) function. \n",
    "- For the simplicity of following analysis, filter only tweets of 2020.\n",
    "- Show the result.\n",
    "\n",
    "Try to match this view:\n",
    "\n",
    "```\n",
    "+-----+----+---+-----------+\n",
    "|month|week|day|    hashtag|\n",
    "+-----+----+---+-----------+\n",
    "|    1|   1|  1| spurfamily|\n",
    "|    1|   1|  1|newyear2020|\n",
    "|    1|   1|  1|     master|\n",
    "|    1|   1|  1|  spurrific|\n",
    "|    1|   1|  1|     master|\n",
    "+-----+----+---+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# daily_hashtag = english_df.<your_code>\n",
    "# Drop unnecessary columns\n",
    "daily_hashtag = english_df.drop(english_df.timestamp_s).drop(english_df.lang)\n",
    "\n",
    "# Create new dataframe with month, week of the year and day of the year\n",
    "daily_hashtag = daily_hashtag.withColumn('month', F.month(english_df.date))\\\n",
    "                             .withColumn('week', F.weekofyear(english_df.date))\\\n",
    "                             .withColumn('day', F.dayofyear(english_df.date))\\\n",
    "                            .withColumn('year', F.year(english_df.date))\n",
    "daily_hashtag = daily_hashtag.filter('year == \"2020\"') ## FILTER IS ADDED HERE\n",
    "daily_hashtag = daily_hashtag.select(F.col('month'), F.col('week'), \n",
    "                                     F.col('day'), F.col('hashtag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the result\n",
    "daily_hashtag.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Daily counts - 2/50\n",
    "\n",
    "Now we want to calculate the number of times a hashtag is used per day based on the dataframe `daily_hashtag`. Sort in descending order of daily counts and show the result. Call the resulting dataframe `day_counts`.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "```\n",
    "+---+----------------------+----+------+\n",
    "|day|hashtag               |week|count |\n",
    "+---+----------------------+----+------+\n",
    "|229|pawankalyanbirthdaycdp|33  |202241|\n",
    "|222|hbdmaheshbabu         |32  |195718|\n",
    "|228|pawankalyanbirthdaycdp|33  |152037|\n",
    "|357|100freeiphone12       |52  |122068|\n",
    "|221|hbdmaheshbabu         |32  |120401|\n",
    "+---+----------------------+----+------+\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<p>Make sure you use <b>cache()</b> when you create <b>day_counts</b> because we will need it in the steps that follow!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# day_counts = daily_hashtag.<your_code>\n",
    "# calculate the number of times a hashtag is used per day and sort in descending order of daily counts.\n",
    "day_counts = daily_hashtag.groupBy('day','hashtag','week')\\\n",
    "                       .agg(F.count('hashtag').alias('count'))\\\n",
    "                       .sort(F.desc(('count'))).cache() #Cache added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the result\n",
    "day_counts.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Weekly average - 2/50\n",
    "\n",
    "To get an idea of which hashtags stay popular for several days, calculate the average number of daily occurences for each week. Sort in descending order and show the top 20.\n",
    "\n",
    "__Note:__\n",
    "- Use the `week` column we created above.\n",
    "- Calculate the weekly average using `F.mean(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# day_counts.<your_code>\n",
    "# calculate the average number of daily occurences for each week and sort in descending order and show the top 20.\n",
    "week_avg = day_counts.groupBy('week', 'hashtag')\\\n",
    "                   .agg(F.mean('count').alias('weekly_occur_avg'))\\\n",
    "                   .sort(F.desc(('weekly_occur_avg')))\n",
    "week_avg.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Ranking - 3/20\n",
    "\n",
    "Window functions are another awesome feature of dataframes. They allow users to accomplish complex tasks using very concise and simple code. \n",
    "\n",
    "Above we computed just the hashtag that had the most occurrences on *any* day. Now lets say we want to know the top tweets for *each* day.  \n",
    "\n",
    "This is a non-trivial thing to compute and requires \"windowing\" our data. I recommend reading this [window functions article](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html) to get acquainted with the idea. You can think of a window function as a fine-grained and more flexible `groupBy`. \n",
    "\n",
    "There are two things we need to define to use window functions:\n",
    "\n",
    "1. the \"window\" to use, based on which columns (partitioning) and how the rows should be ordered \n",
    "2. the computation to carry out for each windowed group, e.g. a max, an average etc.\n",
    "\n",
    "Lets see how this works by example. We will define a window function, `daily_window` that will partition data based on the `day` column. Within each window, the rows will be ordered by the daily hashtag count that we computed above. Finally, we will use the rank function **over** this window to give us the ranking of top tweets. \n",
    "\n",
    "In the end, this is a fairly complicated operation achieved in just a few lines of code! (can you think of how to do this with an RDD??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the window function and the ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_window = Window.partitionBy('day').orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above window function says that we should window the data on the `day` column and order it by count. \n",
    "\n",
    "Now we need to define what we want to compute on the windowed data. We will start by just calculating the daily ranking of hashtags, so we can use the helpful built-in `F.rank()` and sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculating the daily ranking of hashtags\n",
    "daily_rank = F.rank() \\\n",
    "              .over(daily_window) \\\n",
    "              .alias('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the top five hashtags for each day in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# day_counts.<your_code>\n",
    "# Create new dataframe with the top 5 hashtags for each day\n",
    "top_five = day_counts.select(F.col('day'), F.col('count'),\n",
    "                             F.col('hashtag'), daily_rank)# minor modification\n",
    "top_five = top_five.filter(top_five.rank < 6).orderBy('day', 'rank')\n",
    "\n",
    "top_five.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Rolling sum - 5/30\n",
    "\n",
    "With window functions, you can also calculate the statistics of a rolling window. \n",
    "\n",
    "In this question, construct a 7-day rolling window (including the day and 6 days before) to calculate the rolling sum of the daily occurences for each hashtag.\n",
    "\n",
    "Your results should be like:\n",
    "- For the hashtag `covid19`:\n",
    "\n",
    "```\n",
    "+---+----+-----+-------+-----------+\n",
    "|day|week|count|hashtag|rolling_sum|\n",
    "+---+----+-----+-------+-----------+\n",
    "| 42|   7|   85|covid19|         85|\n",
    "| 43|   7|   94|covid19|        179|\n",
    "| 45|   7|  192|covid19|        371|\n",
    "| 46|   7|   97|covid19|        468|\n",
    "| 47|   7|  168|covid19|        636|\n",
    "| 48|   8|  317|covid19|        953|\n",
    "| 49|   8|  116|covid19|        984|\n",
    "| 51|   8|  234|covid19|       1124|\n",
    "| 52|   8|  197|covid19|       1129|\n",
    "| 53|   8|  369|covid19|       1401|\n",
    "+---+----+-----+-------+-----------+\n",
    "```\n",
    "\n",
    "- For the hashtag `bts`:\n",
    "\n",
    "```\n",
    "+---+----+-----+-------+-----------+\n",
    "|day|week|count|hashtag|rolling_sum|\n",
    "+---+----+-----+-------+-----------+\n",
    "|  1|   1| 2522|    bts|       2522|\n",
    "|  2|   1| 1341|    bts|       3863|\n",
    "|  3|   1|  471|    bts|       4334|\n",
    "|  4|   1|  763|    bts|       5097|\n",
    "|  5|   1| 2144|    bts|       7241|\n",
    "|  6|   2| 1394|    bts|       8635|\n",
    "|  7|   2| 1673|    bts|      10308|\n",
    "|  8|   2| 5694|    bts|      13480|\n",
    "|  9|   2| 5942|    bts|      18081|\n",
    "| 10|   2| 5392|    bts|      23002|\n",
    "+---+----+-----+-------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# calculate the rolling sum of the daily occurences for each hashtag.\n",
    "rolling_sum_window = Window.partitionBy('hashtag').orderBy(F.asc('day')).rangeBetween(-6,0)\n",
    "# rs_counts = day_counts.<your_code>\n",
    "rs_counts = day_counts.select(F.col('day'), F.col('week'), F.col('count'),\n",
    "                             F.col('hashtag'), F.sum('count')\\\n",
    "                     .over(rolling_sum_window).alias('rolling_sum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the hashtag covid19\n",
    "rs_counts.filter('hashtag == \"covid19\"').show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the hashtag bts\n",
    "rs_counts.filter('hashtag == \"bts\"').show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) DIY - 15/20\n",
    "\n",
    "Use window functions (or other techniques!) to produce lists of top few trending tweets for each week. What's a __\"trending\"__ tweet? Something that seems to be __suddenly growing very rapidly in popularity__. \n",
    "\n",
    "You should be able to identify, for example, Oscars-related hashtags in week 7 when [the 92nd Academy Awards ceremony took place](https://www.oscars.org/oscars/ceremonies/2020), COVID-related hashtags in week 11 when [WHO declared COVID-19 a pandemic](https://www.who.int/director-general/speeches/detail/who-director-general-s-opening-remarks-at-the-media-briefing-on-covid-19---11-march-2020), and other events like the movement of Black Life Matters in late May, the United States presidential elections, the 2020 American Music Awards, etc.\n",
    "\n",
    "The final listing should be clear and concise and the flow of your analysis should be easy to follow. If you make an implementation that is not immediately obvious, make sure you provide comments either in markdown cells or in comments in the code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Create new dataframe with hashtag counts per week \n",
    "week_counts = daily_hashtag.cache()\n",
    "week_counts = week_counts.groupBy('week','hashtag')\\\n",
    "                         .agg(F.count('hashtag').alias('counts'))\\\n",
    "                         .sort('hashtag','week')\n",
    "\n",
    "\n",
    "# Add new column to dataframe with value from previous week for each hashtag\n",
    "# The first-week data for each hashtag gets the previous value set to null\n",
    "counts_window = Window.partitionBy('hashtag').orderBy(F.asc('week'))\n",
    "week_counts = week_counts.withColumn('prev_value', \n",
    "                                     F.lag(week_counts.counts)\\\n",
    "                                              .over(counts_window))\n",
    "\n",
    "# Calculate the difference for each week compared to previous week\n",
    "# The first week that a hashtag shows, the difference is set to its value \n",
    "week_counts = week_counts.withColumn('diff', \n",
    "                                     F.when(F.isnull(week_counts.counts - week_counts.prev_value), \n",
    "                                                    week_counts.counts)\n",
    "                                              .otherwise((week_counts.counts - week_counts.prev_value)))\n",
    "\n",
    "# Rank the hashtags based on 'diff' column\n",
    "ranks_window = Window.partitionBy('week').orderBy(F.desc('diff'))\n",
    "week_ranks = week_counts.select(F.col('week'), F.col('hashtag'),\n",
    "                                F.col('counts'), F.col('diff'),\n",
    "                                F.rank().over(ranks_window).alias('rank'))\n",
    "\n",
    "# Filter out the top 5 ranks for each week\n",
    "top_week = week_ranks.filter(week_ranks.rank < 6).orderBy('week', 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_week.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the top five hashtags in the first two weeks. Bts appeared in both two weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_week.filter(top_week.week==7).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the oscars is the rank 1 hashtag in week 7. We can speculate that the oscar is being held in the week7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_week.filter(top_week.week==11).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the coronavirus and covid19 are the rank1 and rank2 hashtag in week 11. We can speculate that the covid19 large-scale outbreak all over the world in the week11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_week.filter(top_week.hashtag.contains('bts')).show()\n",
    "top_week.filter(top_week.hashtag.contains('covid')).show()\n",
    "top_week.filter(top_week.hashtag.contains('coronavirus')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that bts and covid19 are hot hashtag that last the whole year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Hashtag clustering (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Feature vector - 3/25\n",
    "\n",
    "- Create a dataframe `daily_hashtag_matrix` that consists of hashtags as rows and daily counts as columns (hint: use `groupBy` and methods of `GroupedData`). Each row of the matrix represents the time series of daily counts of one hashtag. Cache the result.\n",
    "\n",
    "- Create the feature vector which consists of daily counts using the [`VectorAssembler`](https://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) from the Spark ML library. Cache the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# daily_hashtag_matrix = ...\n",
    "# df_vector = ...\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "daily_hashtag_matrix = daily_hashtag.groupBy('hashtag').pivot('day').count().na.fill(0).drop('day').cache()\n",
    "daily_hashtag_matrix.show(10)\n",
    "df_count_cols = ([c for c in daily_hashtag_matrix.columns if c!=\"hashtag\"])\n",
    "vecAssembler = VectorAssembler(inputCols=df_count_cols, outputCol=\"features\")\n",
    "df_vector = vecAssembler.transform(daily_hashtag_matrix)\\\n",
    "                        .select(\"hashtag\",\"features\").cache()\n",
    "df_vector.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Visualization - 2/25\n",
    "\n",
    "Visualize the time sereis you just created. \n",
    "\n",
    "- Select a few interesting hashtags you identified above. `isin` method of DataFrame columns might be useful.\n",
    "- Convert the subset DataFrame as pandas DataFrame with the method `toPandas`.\n",
    "- Plot the time series for the chosen hashtags with matplotlib. To visualize in this PySpark kernel, you need the magic `%matplot plt`. For details, please check [here](https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/screenshots/matplotlib.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# The hashtags we identified above do not have much correlations, which may not be instrumental for explanation,\n",
    "# Here, we pick up the following hashtags, including two US president election candidates (biden and trump), \n",
    "# two great events (covid and blm), two lengendary atheletes unfortunately passing away in 2020 (kobe and maradona)\n",
    "interesting_list=[\"covid\",\"maradona\",\"blm\",\"trump\",\"biden\",\"kobe\"]\n",
    "interesting_hashtags = daily_hashtag_matrix.filter(daily_hashtag_matrix.hashtag.isin(interesting_list))\n",
    "interesting_hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted to Pandas dataframe\n",
    "interesting_hashtag_df = interesting_hashtags.toPandas().set_index('hashtag')\n",
    "interesting_hashtag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the popularity of the hashtags in the course of time\n",
    "ax = interesting_hashtag_df.T.plot(title = 'daily occurences of certain hashtags')\n",
    "ax.set_xlabel(\"day\")\n",
    "ax.set_ylabel(\"daily occurences\")\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) KMeans clustering - 20/25\n",
    "\n",
    "Use KMeans to cluster hashtags based on the daily count timeseries you created above. Train the model and calculate the cluster membership for all hashtags. Again, be creative and see if you can get meaningful hashtag groupings. \n",
    "\n",
    "Validate your results by showing certain clusters, for example, those including some interesting hashtags you identified above. Do they make sense?\n",
    "\n",
    "Make sure you document each step of your process well so that your final notebook is easy to understand even if the result is not optimal or complete. \n",
    "\n",
    "__Note:__ \n",
    "- Additional data cleaning, feature engineering, deminsion reduction, etc. might be necessary to get meaningful results from the model. \n",
    "- For available methods, check [pyspark.sql.functions documentation](https://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html#module-pyspark.sql.functions), [Spark MLlib Guide](https://spark.apache.org/docs/2.3.2/ml-guide.html) and [pyspark.ml documentation](https://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import Normalizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# First, we launch a mini clustering task to analyse those 6 interesting hashtags, with the number of clusters being 5\n",
    "# We would like to put together those hashtags which went through a chronologically similar process in the year\n",
    "# of 2020, which may indicate correlations between them. However, since events vary a lot in their popularity \n",
    "# intrinsically, we first normalize the feature vectors to bring close the events which came and left at the same time,\n",
    "# but differ a lot in popularities. For example, biden and trump should be highly related. Yet, trump is much more\n",
    "# popular than biden for most of the time.\n",
    "interesting_vector = vecAssembler.transform(interesting_hashtags)\\\n",
    "                        .select(\"hashtag\",\"features\").cache()\n",
    "interesting_vector.show()\n",
    "#perform normalization using 1-norm\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(interesting_vector).drop(\"features\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform 5-mean clustering\n",
    "\n",
    "kmeans = KMeans().setK(5).setFeaturesCol(\"normFeatures\").setSeed(1)\n",
    "model_interesting = kmeans.fit(l1NormData)\n",
    "\n",
    "predicted_interesting = model_interesting.transform(l1NormData)\n",
    "predicted_interesting.show()\n",
    "# As can be seen from the result below. Were the six hashtags forcefully catagorized into 5 groups, trump and biden\n",
    "# are in one group, which makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we perform the similar workflow to the entire hashtag dataframe\n",
    "# normalize everything\n",
    "df_vector_norm = normalizer.transform(df_vector).drop(\"features\").cache()\n",
    "df_vector_norm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_norm = df_vector_norm.withColumnRenamed(\"normFeatures\",\"features\")\n",
    "df_vector_norm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of K\n",
    "\n",
    "# We will try to select a proper K with the help of the 'cost' yielded by different clustering K's.\n",
    "# Note, this part may take a very long time to run. To make things easier, after experiments, we adopt the following \n",
    "# strategy: We only investigate K equaling to multiples of 100 and smaller than 1000. For every K, we fit two models \n",
    "# and get the mean cost of the two models.\n",
    "\n",
    "\n",
    "K_selection_num = 9\n",
    "start_K = 100  \n",
    "cost = np.zeros(K_selection_num)\n",
    "tested_K = np.zeros(K_selection_num)\n",
    "\n",
    "for point in range(K_selection_num):\n",
    "    K = start_K + point * 100\n",
    "    kmeans = KMeans().setK(K).setSeed(1)\n",
    "    model1 = kmeans.fit(df_vector_norm.sample(0.01, seed = 1))\n",
    "    model2 = kmeans.fit(df_vector_norm.sample(0.01, seed = 2))\n",
    "    cost1 = model1.computeCost(df_vector_norm)\n",
    "    cost2 = model2.computeCost(df_vector_norm)\n",
    "    tested_K[point] = K\n",
    "    cost[point] = np.mean([cost1, cost2])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(tested_K,cost)\n",
    "ax.set_xlabel('k', fontsize = 10)\n",
    "ax.set_ylabel('cost', fontsize = 10)\n",
    "ax.set_title('Cost of the Kmeans algorithm w.r.t the value of K', fontsize = 12)\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# From the plot below, we can see that the cost almost levels off at about k=600. Thus, we use this value for \n",
    "# model fitting.\n",
    "# Note that we can tell that the 'elbow point' of this clustering process is at around 400. However, from later analysis,\n",
    "# We can tell that even clustering number of 600 is not 'fine-grained' enough. Perhaps this rough analysis of the \n",
    "# hashtags, without taking other meta information into consideration, is less effective. \n",
    "%matplot plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we fit the model and transform the dataframe\n",
    "kmeans = KMeans().setK(600).setSeed(2)\n",
    "model = kmeans.fit(df_vector_norm.sample(0.01, seed = 2))\n",
    "transformed = model.transform(df_vector_norm)\n",
    "transformed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then revisit the clustering process of the six interesting hashtags which we picked out.\\n\",\n",
    "interesting_rows = transformed.filter(transformed.hashtag.isin([\"covid\",\"maradona\",\"blm\",\"trump\",\"biden\",\"kobe\"]))\n",
    "interesting_rows.show(10)\n",
    "# But disappointedly, as is shown below, the categorization is not satisfying. We think the crudeness of clusters\n",
    "# is to be blamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we did detect meaningful clustering in the imperfect result\n",
    "# For example, the four hashtag related to the oscar award are brought together in the clustering process.\n",
    "# P.S. the second and the third hashtags are the recipient of the Best Picture Award of Oscar\n",
    "oscar_row = transformed.filter(transformed.hashtag.isin(['oscar','기생충','parasite','92ndacademyawards']))\n",
    "oscar_row.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all, folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
