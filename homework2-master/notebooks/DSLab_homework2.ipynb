{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Data Wrangling with Hadoop\n",
    "\n",
    "The goal of this assignment is to put into action the data wrangling techniques from the exercises of week-3 and week-4. We highly suggest you to finish these two exercises first and then start the homework. In this homework, we are going to reuse the same __sbb__ and __twitter__ datasets as seen before in these two exercises. \n",
    "\n",
    "## Hand-in Instructions\n",
    "- __Due: 13.04.2021 23:59 CET__\n",
    "- `git push` your final verion to your group's Renku repository before the due date\n",
    "- Verify that `Dockerfile`, `environment.yml` and `requirements.txt` are properly written and notebook is functional\n",
    "- Add necessary comments and discussion to make your queries readable\n",
    "\n",
    "## Hive Documentation\n",
    "\n",
    "Hive queries: <https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select>\n",
    "\n",
    "Hive functions: <https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 150%\" class=\"alert alert-block alert-warning\">\n",
    "    <b>Get yourself ready:</b> \n",
    "    <br>\n",
    "    Before you jump into the questions, please first go through the notebook <a href='./prepare_env.ipynb'>prepare_env.ipynb</a> and make sure that your environment is properly set up.\n",
    "    <br><br>\n",
    "    <b>Cluster Usage:</b>\n",
    "    <br>\n",
    "    As there are many of you working with the cluster, we encourage you to prototype your queries on small data samples before running them on whole datasets.\n",
    "    <br><br>\n",
    "    <b>Try to use as much HiveQL as possible and avoid using pandas operations.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: SBB/CFF/FFS Data (40 Points)\n",
    "\n",
    "Data source: <https://opentransportdata.swiss/en/dataset/istdaten>\n",
    "\n",
    "In this part, you will leverage Hive to perform exploratory analysis of data published by the [Open Data Platform Swiss Public Transport](https://opentransportdata.swiss).\n",
    "\n",
    "Format: the dataset is originally presented as a collection of textfiles with fields separated by ';' (semi-colon). For efficiency, the textfiles have been compressed into Optimized Row Columnar ([ORC](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC)) file format. \n",
    "\n",
    "Location: you can find the data in ORC format on HDFS at the path `/data/sbb/orc/istdaten`.\n",
    "\n",
    "The full description from opentransportdata.swiss can be found in <https://opentransportdata.swiss/de/cookbook/ist-daten/> in four languages. Because of the translation typos there may be some misunderstandings. We suggest you rely on the German version and use an automated translator when necessary. We will clarify if there is still anything unclear in class and Slack. Here are the relevant column descriptions:\n",
    "\n",
    "- `BETRIEBSTAG`: date of the trip\n",
    "- `FAHRT_BEZEICHNER`: identifies the trip\n",
    "- `BETREIBER_ABK`, `BETREIBER_NAME`: operator (name will contain the full name, e.g. Schweizerische Bundesbahnen for SBB)\n",
    "- `PRODUKT_ID`: type of transport, e.g. train, bus\n",
    "- `LINIEN_ID`: for trains, this is the train number\n",
    "- `LINIEN_TEXT`,`VERKEHRSMITTEL_TEXT`: for trains, the service type (IC, IR, RE, etc.)\n",
    "- `ZUSATZFAHRT_TF`: boolean, true if this is an additional trip (not part of the regular schedule)\n",
    "- `FAELLT_AUS_TF`: boolean, true if this trip failed (cancelled or not completed)\n",
    "- `HALTESTELLEN_NAME`: name of the stop\n",
    "- `ANKUNFTSZEIT`: arrival time at the stop according to schedule\n",
    "- `AN_PROGNOSE`: actual arrival time\n",
    "- `AN_PROGNOSE_STATUS`: show how the actual arrival time is calcluated\n",
    "- `ABFAHRTSZEIT`: departure time at the stop according to schedule\n",
    "- `AB_PROGNOSE`: actual departure time\n",
    "- `AB_PROGNOSE_STATUS`: show how the actual departure time is calcluated\n",
    "- `DURCHFAHRT_TF`: boolean, true if the transport does not stop there\n",
    "\n",
    "Each line of the file represents a stop and contains arrival and departure times. When the stop is the start or end of a journey, the corresponding columns will be empty (`ANKUNFTSZEIT`/`ABFAHRTSZEIT`).\n",
    "\n",
    "In some cases, the actual times were not measured so the `AN_PROGNOSE_STATUS`/`AB_PROGNOSE_STATUS` will be empty or set to `PROGNOSE` and `AN_PROGNOSE`/`AB_PROGNOSE` will be empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "hiveaddr = os.environ['HIVE_SERVER_2']\n",
    "print(\"Operating as: {0}\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "# create connection\n",
    "conn = hive.connect(host=hiveaddr, \n",
    "                    port=10000,\n",
    "                    username=username) \n",
    "# create cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Type of transport - 10/40\n",
    "\n",
    "In the exercise of week-3, you have already explored the stop distribution of different types of transport on 05.11.2018. Now, let's do the same for the whole dataset.\n",
    "\n",
    "- Query `sbb_orc` to get the total number of stops for different types of transport in each month, and order it by time and type of transport.\n",
    "|month_year|ttype|stops|\n",
    "|---|---|---|\n",
    "|...|...|...|\n",
    "- Use `plotly` to create a facet bar chart partitioned by the type of transportation. \n",
    "- Document any patterns or abnormalities you can find.\n",
    "\n",
    "__Note__: \n",
    "- In general, one entry in the `sbb_orc` table means one stop.\n",
    "- You might need to filter out the rows where:\n",
    "    - `BETRIEBSTAG` is not in the format of `__.__.____`\n",
    "    - `PRODUKT_ID` is NULL or empty\n",
    "- Facet plot with plotly: https://plotly.com/python/facet-plots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select from_unixtime(unix_timestamp(BETRIEBSTAG, 'dd.MM.yyyy'), 'yyyy-MM') as month_year, \n",
    "        lower(PRODUKT_ID) as ttype,\n",
    "        count(*) as stops\n",
    "    from {0}.sbb_orc\n",
    "    where BETRIEBSTAG != '__.__.____' and PRODUKT_ID is not null and length(PRODUKT_ID) > 0 and length(BETRIEBSTAG)>0 and date_format(to_date(from_unixtime(unix_timestamp(BETRIEBSTAG, 'dd.MM.yyyy'))), 'yyyy-MM') is not null\n",
    "    group by lower(PRODUKT_ID), from_unixtime(unix_timestamp(BETRIEBSTAG, 'dd.MM.yyyy'), 'yyyy-MM')\n",
    "    order by month_year asc\n",
    "\"\"\".format(username)\n",
    "df_ttype = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table\n",
    "df_ttype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_ttype,\n",
    "    x=\"month_year\",\n",
    "    y=\"stops\",\n",
    "    color=\"ttype\",\n",
    "    labels={\"month_year\": \"Months\",\n",
    "            \"stops\": \"Number of stops\",\n",
    "            \"ttype\": \"Types of transport\"\n",
    "           },\n",
    "    title=\"The stop distribution of different types of transports over different months\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documents:** According to the table, we find that bus, train(zug), tram are the most commonly used types of transportation. For each type of transportaion, the number of stops of bus is increasing year by year from 2018 to 2020; the usage of schiff reaches its peak at every August; the use of zahnradbahn is on the rise and is slowly surpassing the use of metros from 2020. Otherwise, we find the stops in July 2019 is significantly less than other months for all the types of transportation except for ahnradbahn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Schedule - 10/40\n",
    "\n",
    "- Select a typical week day (not Saturday, not Sunday, not a bank holiday) from `sbb_orc`. Query the one-day table and get the set of IC (`VERKEHRSMITTEL_TEXT`) trains you can take to go (without connections) from Genève to Lausanne on that day. \n",
    "- Display the train number (`LINIEN_ID`) as well as the schedule (arrival and departure time) of the trains.\n",
    "\n",
    "|train_number|departure|arrival|\n",
    "|---|---|---|\n",
    "|...|...|...|\n",
    "\n",
    "__Note:__ \n",
    "- The schedule of IC from Genève to Lausanne has not changed for the past few years. You can use the advanced search of SBB's website to check your answer.\n",
    "- Do not hesitate to create intermediary tables. \n",
    "- You might need to add filters on these flags: `ZUSATZFAHRT_TF`, `FAELLT_AUS_TF`, `DURCHFAHRT_TF` \n",
    "- Functions that could be useful: `unix_timestamp`, `to_utc_timestamp`, `date_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.geneve_station\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create a table for IC trains in Geneva for a week day (e.g. 10/10/2019)\n",
    "query = \"\"\"\n",
    "    create table {0}.geneve_station\n",
    "    stored as orc\n",
    "    as \n",
    "        select *\n",
    "        from {0}.sbb_orc\n",
    "        where VERKEHRSMITTEL_TEXT like 'IC' \n",
    "        and HALTESTELLEN_NAME like 'Genève' \n",
    "        and BETRIEBSTAG like '10.10.2019'\n",
    "        and FAELLT_AUS_TF like 'false'\n",
    "        and DURCHFAHRT_TF like 'false'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Drop table if exists\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.lausanne_station\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create a table for IC trains in Lausanne for a week day (e.g. 10/10/2019)\n",
    "query = \"\"\"\n",
    "    create table {0}.lausanne_station\n",
    "    stored as orc\n",
    "    as \n",
    "        select *\n",
    "        from {0}.sbb_orc\n",
    "        where VERKEHRSMITTEL_TEXT like 'IC' \n",
    "        and HALTESTELLEN_NAME like 'Lausanne' \n",
    "        and BETRIEBSTAG like '10.10.2019'\n",
    "        and FAELLT_AUS_TF like 'false'\n",
    "        and DURCHFAHRT_TF like 'false'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select  \n",
    "        geneve_station.LINIEN_ID as train_number, \n",
    "        to_utc_timestamp(1000*unix_timestamp(geneve_station.AB_PROGNOSE, 'dd.MM.yyy HH:mm'),\"UTC\") as departure,\n",
    "        to_utc_timestamp(1000*unix_timestamp(lausanne_station.AN_PROGNOSE, 'dd.MM.yyy HH:mm'),\"UTC\") as arrival  \n",
    "    from {0}.geneve_station FULL OUTER JOIN {0}.lausanne_station ON (geneve_station.LINIEN_ID = lausanne_station.LINIEN_ID)\n",
    "    where \n",
    "        unix_timestamp(lausanne_station.AN_PROGNOSE, 'dd.MM.yyy HH:mm') > 0 and \n",
    "        unix_timestamp(geneve_station.AB_PROGNOSE, 'dd.MM.yyy HH:mm') > 0 and \n",
    "        unix_timestamp(lausanne_station.AN_PROGNOSE, 'dd.MM.yyy HH:mm') > unix_timestamp(geneve_station.AB_PROGNOSE, 'dd.MM.yyy HH:mm')\n",
    "    order by geneve_station.LINIEN_ID\n",
    "\"\"\".format(username)\n",
    "df_geneve_lausanne = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table\n",
    "df_geneve_lausanne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Delay percentiles - 10/40\n",
    "\n",
    "- Query `sbb_orc` to compute the 50th and 75th percentiles of __arrival__ delays for IC 702, 704, ..., 728, 730 (15 trains total) at Genève main station. \n",
    "- Use `plotly` to plot your results in a proper way. \n",
    "- Which trains are the most disrupted? Can you find the tendency and interpret?\n",
    "\n",
    "__Note:__\n",
    "- Do not hesitate to create intermediary tables. \n",
    "- When the train is ahead of schedule, count this as a delay of 0.\n",
    "- Use only stops with `AN_PROGNOSE_STATUS` equal to __REAL__ or __GESCHAETZT__.\n",
    "- Functions that may be useful: `unix_timestamp`, `percentile_approx`, `if`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.geneva_main_station\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Select trains\n",
    "query = \"\"\"\n",
    "    create table {0}.geneva_main_station\n",
    "    stored as orc\n",
    "    as \n",
    "        select LINIEN_ID as id,\n",
    "               unix_timestamp(ANKUNFTSZEIT, 'dd.MM.yyyy HH:mm') as expected_arrival, \n",
    "               unix_timestamp(AN_PROGNOSE, 'dd.MM.yyyy HH:mm:ss') as actual_arrival\n",
    "        from {0}.sbb_orc\n",
    "        where \n",
    "            VERKEHRSMITTEL_TEXT like 'IC' and \n",
    "            HALTESTELLEN_NAME like 'Genève' and \n",
    "            (AN_PROGNOSE_STATUS like 'REAL' or AN_PROGNOSE_STATUS like 'GESCHAETZT') and \n",
    "            cast(LINIEN_ID as int) >= 702 and \n",
    "            cast(LINIEN_ID as int) <= 730 and \n",
    "            (cast(LINIEN_ID as int)% 2) == 0\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the delay of each entry\n",
    "query = \"\"\"\n",
    "   select id, \n",
    "        if (actual_arrival > expected_arrival, actual_arrival - expected_arrival, 0) as delay\n",
    "    from {0}.geneva_main_station\n",
    "    order by delay asc\n",
    "\"\"\".format(username)\n",
    "df_delays_ic_gen=pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 50th and 75th percentiles of arrival delays for different IC trains\n",
    "query=\"\"\"\n",
    "with delay_IC as(\n",
    "    select id, \n",
    "           from_unixtime(expected_arrival, 'HH:mm') as schedule_arrival,\n",
    "           if (actual_arrival > expected_arrival, actual_arrival - expected_arrival, 0) as delay\n",
    "    from {0}.geneva_main_station\n",
    "    order by delay asc\n",
    ")\n",
    "select id,\n",
    "       percentile_approx(delay,0.5) as 50th,\n",
    "       percentile_approx(delay,0.75) as 75th,\n",
    "       schedule_arrival\n",
    "       from delay_IC\n",
    "       group by id, schedule_arrival\n",
    "       order by id\n",
    "\"\"\".format(username)\n",
    "df_delays_ic_gen_each = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table\n",
    "df_delays_ic_gen_each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('50th percentile of delays (in seconds) over all trains:')\n",
    "df_delays_ic_gen['delay'].quantile(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('75th percentile of delays (in seconds) over all trains:')\n",
    "df_delays_ic_gen['delay'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delays_ic_gen_each=pd.melt(df_delays_ic_gen_each,id_vars=['id', 'schedule_arrival'],var_name='percentile', value_name='value')\n",
    "fig = px.bar(\n",
    "    df_delays_ic_gen_each,\n",
    "    x=\"id\",\n",
    "    y='value',\n",
    "    color='percentile',\n",
    "    barmode='group',\n",
    "    labels={\"id\": \"Train number\",\n",
    "            \"percentile\": \"Percentile\",\n",
    "            \"value\": \"Delays (in seconds)\"\n",
    "           },\n",
    "    title=\"The delay distribution of different train numbers\",\n",
    ")\n",
    "\n",
    "fig.update_xaxes(type='category', categoryorder='category ascending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: According to 75th percentiles, train 708 is most disrupted. According to 50th percentiles, train 726 is most disrupted. Therefore, train 708 and 726 are most disrupted. We find that trains run during morning and evening peak are more prone to delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Delay heatmap 10/40\n",
    "\n",
    "- For each week (1 to 52) of each year (2018 to 2020), query `sbb_orc` to compute the median of delays of all trains __departing__ from any train stations in Zürich area during that week. \n",
    "- Use `plotly` to draw a heatmap (year x week) of the median delays. \n",
    "- In which weeks were the trains delayed the most/least? Can you explain the results?\n",
    "\n",
    "__Note:__\n",
    "- Do not hesitate to create intermediary tables. \n",
    "- When the train is ahead of schedule, count this as a delay of 0.\n",
    "- Use only stops with `AB_PROGNOSE_STATUS` equal to __REAL__ or __GESCHAETZT__.\n",
    "- For simplicty, a train station in Zürich area <=> it's a train station & its `HALTESTELLEN_NAME` starts with __Zürich__.\n",
    "- Heatmap with `plotly`: https://plotly.com/python/heatmaps/\n",
    "- Functions that may be useful: `unix_timestamp`, `from_unixtime`, `weekofyear`, `percentile_approx`, `if`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.zurich_station\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query = \"\"\"\n",
    "    create table {0}.zurich_station\n",
    "    stored as orc\n",
    "    as \n",
    "        select LINIEN_ID as id,\n",
    "               weekofyear(TO_DATE(FROM_UNIXTIME(unix_timestamp(BETRIEBSTAG, 'dd.MM.yyyy')))) as week,\n",
    "               Year(TO_DATE(FROM_UNIXTIME(unix_timestamp(BETRIEBSTAG, 'dd.MM.yyyy')))) as year,\n",
    "               unix_timestamp(ABFAHRTSZEIT, 'dd.MM.yyyy HH:mm') as expected_departure, \n",
    "               unix_timestamp(AB_PROGNOSE, 'dd.MM.yyyy HH:mm:ss') as actual_departure\n",
    "        from {0}.sbb_orc\n",
    "        where HALTESTELLEN_NAME like 'Zürich%' \n",
    "        and PRODUKT_ID like 'Zug'\n",
    "        and (AB_PROGNOSE_STATUS like 'REAL' or AB_PROGNOSE_STATUS like 'GESCHAETZT')\n",
    "        \n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with delay_zurich as(\n",
    "select id, week, year,\n",
    "        case\n",
    "            when actual_departure > expected_departure then actual_departure - expected_departure\n",
    "            else 0\n",
    "        end as delay\n",
    "    from {0}.zurich_station\n",
    "    order by delay asc\n",
    ")\n",
    "select week, year,percentile_approx(delay,0.5) as medians\n",
    "from delay_zurich\n",
    "group by week, year\n",
    "order by year, week\n",
    "\"\"\".format(username)\n",
    "df_delays_zurich = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_delays_zurich = df_delays_zurich.set_index(['year', 'week'])\n",
    "# Fill up missed data\n",
    "add_indices = pd.Index((i, j) for i in range(2018, 2021) for j in range(1, 53)).difference(df_delays_zurich.index)\n",
    "add_df = pd.DataFrame(index=add_indices, columns=df_delays_zurich.columns).fillna(np.NaN)\n",
    "df_delays_zurich = pd.concat([df_delays_zurich, add_df])\n",
    "# Delete 53th week in 2020 to keep 52 weeks each year\n",
    "df_delays_zurich = df_delays_zurich.drop((2020, 53))\n",
    "df_delays_zurich = df_delays_zurich.loc[[(i, j) for i in range(2018, 2021) for j in range(1, 53)], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table\n",
    "df_delays_zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    df_delays_zurich.to_numpy().reshape((3, -1)),\n",
    "    x = [str(i) for i in range (1, 53)],\n",
    "    y = [str(i) for i in range(2018, 2021)],\n",
    "    aspect = 'auto',\n",
    "    labels=dict(x=\"Week\", y=\"Year\", color=\"Delays (in second)\"),\n",
    "    title='Median of delays (in second) of all trains for different week',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: According to the heatmap, in the 44th week of 2019 (Oct. 28 to Nov. 3), the trains delayed the most. Reasons might be: 1)Federal elections were held in Switzerland on 20 October 2019 to elect all members of both houses of the Federal Assembly; 2) Daylight saving time ends in late October 2019.  \n",
    "In the 14th week of 2020 (Mar. 30 to Apr. 5), the trains delayed the least. Reasons might be: 1) Because of the Covid situation, people started to work remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Twitter Data (20 Points)\n",
    "\n",
    "Data source: https://archive.org/details/twitterstream?sort=-publicdate \n",
    "\n",
    "In this part, you will leverage Hive to extract the hashtags from the source data, and then perform light exploration of the prepared data. \n",
    "\n",
    "### Dataset Description \n",
    "\n",
    "Format: the dataset is presented as a collection of textfiles containing one JSON document per line. The data is organized in a hierarchy of folders, with one file per minute. The textfiles have been compressed using bzip2. In this part, we will mainly focus on __2020 twitter data__.\n",
    "\n",
    "Location: you can find the data on HDFS at the path `/data/twitter/json/2020/{month}/{day}/{hour}/{minute}.json.bz2`. \n",
    "\n",
    "Relevant fields: \n",
    "- `created_at`, `timestamp_ms`: The first is a human-readable string representation of when the tweet was posted. The latter represents the same instant as a timestamp in seconds since UNIX epoch. \n",
    "- `lang`: the language of the tweet content \n",
    "- `entities`: parsed entities from the tweet, e.g. hashtags, user mentions, URLs.\n",
    "- In this repository, you can find [a tweet example](../data/tweet-example.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 100%\" class=\"alert alert-block alert-danger\">\n",
    "    <b>Disclaimer</b>\n",
    "    <br>\n",
    "    This dataset contains unfiltered data from Twitter. As such, you may be exposed to tweets/hashtags containing vulgarities, references to sexual acts, drug usage, etc.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) JsonSerDe - 4/20\n",
    "\n",
    "In the exercise of week 4, you have already seen how to use the [SerDe framework](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&SerDe) to extract JSON fields from raw text format. \n",
    "\n",
    "In this question, please use SerDe to create an <font color=\"red\" size=\"3px\">EXTERNAL</font> table with __one day__ twitter data. You only need to extract three columns: `timestamp_ms`, `lang` and `entities`(with the field `hashtags` only) with following schema (you need to figure out what to fill in `...`):\n",
    "```\n",
    "timestamp_ms string,\n",
    "lang         string,\n",
    "entities     struct<hashtags:array<...<text:..., indices:...>>>\n",
    "```\n",
    "\n",
    "The table you create should be similar to:\n",
    "\n",
    "| timestamp_ms | lang | entities |\n",
    "|---|---|---|\n",
    "| 1234567890001 | en | {\"hashtags\":[]} |\n",
    "| 1234567890002 | fr | {\"hashtags\":[{\"text\":\"hashtag1\",\"indices\":[10]}]} |\n",
    "| 1234567890002 | jp | {\"hashtags\":[{\"text\":\"hashtag1\",\"indices\":[14,23]}, {\"text\":\"hashtag2\",\"indices\":[45]}]} |\n",
    "\n",
    "__Note:__\n",
    "   - JsonSerDe: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&SerDe\n",
    "   - Hive data types: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query=\"\"\"\n",
    "    drop table if exists {0}.hashtags_one_day_intermediary\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create a external table with one day twitter data (e.g. 10/10/2020)\n",
    "query=\"\"\"\n",
    "    create external table {0}.hashtags_one_day_intermediary(    \n",
    "        timestamp_ms string,\n",
    "        lang string,\n",
    "        entities struct<hashtags:array<struct<text:string, indices:array<int>>>>\n",
    "    )\n",
    "    row format serde 'org.apache.hadoop.hive.serde2.JsonSerDe'\n",
    "    stored as textfile\n",
    "    location '/data/twitter/json/2020/10/10'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Show the table\n",
    "query=\"\"\"\n",
    "    select * from {0}.hashtags_one_day_intermediary limit 10\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Explosion - 4/20\n",
    "\n",
    "In a), you create a table where each row could contain a list of multiple hashtags. Create another table by normalizing the table obtained from the previous step. This means that each row should contain exactly one hashtag. Include `timestamp_ms` and `lang` in the resulting table, as shown below.\n",
    "\n",
    "| timestamp_ms | lang | hashtag |\n",
    "|---|---|---|\n",
    "| 1234567890001 | es | hashtag1 |\n",
    "| 1234567890001 | es | hashtag2 |\n",
    "| 1234567890002 | en | hashtag2 |\n",
    "| 1234567890003 | zh | hashtag3 |\n",
    "\n",
    "__Note:__\n",
    "   - `LateralView`: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView\n",
    "   - `explode` function: <https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-explode>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query=\"\"\"\n",
    "    drop table if exists {0}.hashtags_one_day\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create normalized table\n",
    "query=\"\"\"\n",
    "    create table if not exists {0}.hashtags_one_day\n",
    "    stored as orc\n",
    "    as\n",
    "        select timestamp_ms, lang, hashtag\n",
    "        from {0}.hashtags_one_day_intermediary lateral view explode(entities.hashtags.text) adTable as hashtag\n",
    "        \n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Show the table\n",
    "query=\"\"\"\n",
    "    select * from {0}.hashtags_one_day limit 10\n",
    "\"\"\".format(username)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Hashtags - 8/20\n",
    "\n",
    "Query the normailized table you obtained in b). Create a table of the top 20 most mentioned hashtags with the contribution of each language. And, for each hashtag, order languages by their contributions. You should have a table similar to:\n",
    "\n",
    "|hashtag|lang|lang_count|total_count|\n",
    "|---|---|---|---|\n",
    "|hashtag_1|en|2000|3500|\n",
    "|hashtag_1|fr|1000|3500|\n",
    "|hashtag_1|jp|500|3500|\n",
    "|hashtag_2|te|500|500|\n",
    "\n",
    "Use `plotly` to create a stacked bar chart to show the results.\n",
    "\n",
    "__Note:__ to properly order the bars, you may need:\n",
    "```python\n",
    "fig.update_layout(xaxis_categoryorder = 'total descending')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query=\"\"\"\n",
    "    drop table if exists {0}.count_by_lang\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create table to count by language\n",
    "query=\"\"\"\n",
    "    create table if not exists {0}.count_by_lang\n",
    "    stored as orc\n",
    "    as\n",
    "        select hashtag, lang, count(*) as lang_count\n",
    "        from {0}.hashtags_one_day\n",
    "        group by hashtag, lang\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query=\"\"\"\n",
    "    drop table if exists {0}.count_by_hashtag\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create table to count by hashtag\n",
    "query=\"\"\"\n",
    "    create table if not exists {0}.count_by_hashtag\n",
    "    stored as orc\n",
    "    as\n",
    "        select hashtag, lang, lang_count, \n",
    "        sum(lang_count) over (partition by hashtag) as total_count\n",
    "        from {0}.count_by_lang\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query=\"\"\"\n",
    "    drop table if exists {0}.top_hashtags_one_day\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create table for ranking\n",
    "query=\"\"\"\n",
    "    create table if not exists {0}.top_hashtags_one_day\n",
    "    stored as orc\n",
    "    as\n",
    "        select *,\n",
    "        dense_rank() over(order by total_count desc) as rank\n",
    "        from {0}.count_by_hashtag\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query=\"\"\"\n",
    "    drop table if exists {0}.top20_hashtags_one_day\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "# Create table to select top 20\n",
    "query=\"\"\"\n",
    "    create table if not exists {0}.top20_hashtags_one_day\n",
    "    stored as orc\n",
    "    as \n",
    "        select hashtag, lang, lang_count, total_count\n",
    "        from {0}.top_hashtags_one_day \n",
    "        where rank<=20\n",
    "        sort by total_count desc, lang_count desc\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query=\"\"\"\n",
    "    select * from {0}.top20_hashtags_one_day \n",
    "\"\"\".format(username)\n",
    "df_hashtag = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table\n",
    "df_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_hashtag,\n",
    "    x='top20_hashtags_one_day.hashtag',\n",
    "    y='top20_hashtags_one_day.lang_count',\n",
    "    color='top20_hashtags_one_day.lang',\n",
    "    labels={'top20_hashtags_one_day.hashtag':'Hashtag',\n",
    "            'top20_hashtags_one_day.lang_count':'Number of tweets',\n",
    "            'top20_hashtags_one_day.lang':'Language'\n",
    "            },\n",
    "    title='Top 20 most mentioned hashtags with the contribution of each language'\n",
    ")\n",
    "\n",
    "fig.update_layout(xaxis_categoryorder = 'total descending')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) HBase - 4/20\n",
    "\n",
    "In the lecture and exercise of week-4, you have learnt what's HBase, how to create an Hbase table and how to create an external Hive table on top of the HBase table. Now, let's try to save the results of question c) into HBase, such that each entry looks like:\n",
    "```\n",
    "(b'PIE', {b'cf1:total_count': b'31415926', b'cf2:langs': b'ja,en,ko,fr'})\n",
    "``` \n",
    "where the key is the hashtag, `total_count` is the total count of the hashtag, and `langs` is a string of language abbreviations concatenated with commas. \n",
    "\n",
    "__Note:__\n",
    "- To accomplish the task, you need to follow these steps:\n",
    "    - Create an Hbase table called `twitter_hbase`, in **your hbase namespace**, with two column families and fields (cf1, cf2)\n",
    "    - Create an external Hive table called `twitter_hive_on_hbase` on top of the Hbase table. \n",
    "    - Populate the HBase table with the results of question c).\n",
    "- You may find function `concat_ws` and `collect_list` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "hbaseaddr = os.environ['HBASE_SERVER']\n",
    "hbase_connection = happybase.Connection(hbaseaddr, transport='framed',protocol='compact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hbase_connection.delete_table('{0}:twitter_hbase'.format(username),disable=True)\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "hbase_connection.create_table(\n",
    "    '{0}:twitter_hbase'.format(username),\n",
    "    {'cf1': dict(),\n",
    "     'cf2': dict()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists\n",
    "query = \"\"\"\n",
    "drop table {0}.twitter_hive_on_hbase\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query = \"\"\"\n",
    "create external table {0}.twitter_hive_on_hbase(\n",
    "    RowKey string,\n",
    "    total_count bigint,\n",
    "    langs string\n",
    ")\n",
    "STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n",
    "WITH SERDEPROPERTIES (\n",
    "    \"hbase.columns.mapping\"=\":key,cf1:total_count,cf2:langs\"\n",
    ")\n",
    "TBLPROPERTIES(\n",
    "    \"hbase.table.name\"=\"{0}:twitter_hbase\",\n",
    "    \"hbase.mapred.output.outputtable\"=\"{0}:twitter_hbase\"\n",
    ")\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query=\"\"\"\n",
    "insert overwrite table {0}.twitter_hive_on_hbase\n",
    "    select\n",
    "         hashtag as RowKey,\n",
    "         total_count as total_count,\n",
    "         concat_ws(',',collect_set(cast(lang as string))) as langs\n",
    "    from {0}.top20_hashtags_one_day\n",
    "    group by total_count, hashtag\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table\n",
    "for i, r in enumerate(hbase_connection.table('{0}:twitter_hbase'.format(username)).scan()):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all, folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
